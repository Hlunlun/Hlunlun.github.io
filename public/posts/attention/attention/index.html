<!DOCTYPE html>
<html
  lang="en-us"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  



  
  


<head lang="en-us"><script src="/Hlunlun/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=Hlunlun/livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="color-scheme" content="dark light">
  <meta name="description" content="實作參考Annotated Transformer: Implementation and Concept of Attention" />
  <meta name="author" content="Lun">
  <meta name="keywords" content="Attention">
  <title>Calculate Attention | Lun&#39;s</title>
  <link rel="canonical" href="http://localhost:1313/Hlunlun/posts/attention/attention/" />
  

  
  <meta property="og:type" content="article" />
  <meta property="og:description" content="實作參考Annotated Transformer: Implementation and Concept of Attention" />
  <meta property="og:title" content="Calculate Attention" />
  <meta property="og:site_name" content="Lun&#39;s" />
  <meta property="og:image:type" content="image/jpeg" />
  <meta property="og:url" content="http://localhost:1313/Hlunlun/posts/attention/attention/" />
  <meta property="og:locale" content="en-us" />

  
    <meta property="article:published_time" content="2024-12-22" />
    <meta property="article:modified_time" content="2024-12-22" />
     
      <meta property="article:tag" content="Attention" />
     
  

  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Calculate Attention | Lun&#39;s" />
  <meta name="twitter:description" content="實作參考Annotated Transformer: Implementation and Concept of Attention" />
  <meta name="twitter:domain" content="http://localhost:1313/Hlunlun/posts/attention/attention/" />

  
  
    <link rel="icon" href="http://localhost:1313/Hlunlun/favicon.ico">
  
  
  
  

  
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/modern-normalize/modern-normalize.min.css">

  
  
  

  

  
    <link rel="stylesheet" href="http://localhost:1313/Hlunlun/style.css" rel="preload stylesheet" as="style"/>
  

  
  
</head>

  <body>
    <header id="header">
  <div class="row">
    <div class="container has-padding nav">
      <button id="navbar-toggler" class="navbar-button" aria-hidden="true">











<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M64 384h384v-42.67H64zm0-106.67h384v-42.66H64zM64 128v42.67h384V128z'/></svg>





</button>
      <div class="navbar-brand">
        <a class="logo navbar-button" href="http://localhost:1313/Hlunlun/" title="Lun&#39;s">
          <span>Lun&#39;s</span>
        </a>
      </div>
      <nav class="navbar" role="navigation">
        <ul>
          
          
            <li class="nav-bar-item active">
              <a class="nav-link navbar-button" href="/Hlunlun/posts/" title="posts">
                <span>posts</span>
              </a>
            </li>
          
            <li class="nav-bar-item">
              <a class="nav-link navbar-button" href="/Hlunlun/tags/" title="tags">
                <span>tags</span>
              </a>
            </li>
          
            <li class="nav-bar-item">
              <a class="nav-link navbar-button" href="/Hlunlun/archives/" title="archives">
                <span>archives</span>
              </a>
            </li>
          
            <li class="nav-bar-item">
              <a class="nav-link navbar-button" href="/Hlunlun/about/" title="about">
                <span>about</span>
              </a>
            </li>
          
        </ul>
      </nav>
      <div class="theme-selector">
        <button class="button is-text" id="theme-selector-button" title="toggle theme">
          <span class="label icon">





<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M256 32C132.29 32 32 132.29 32 256s100.29 224 224 224 224-100.29 224-224S379.71 32 256 32zM128.72 383.28A180 180 0 01256 76v360a178.82 178.82 0 01-127.28-52.72z'/></svg>











</span>
        </button>
      </div>
    </div>
    
      <div class="container has-padding">
        <div class="breadcrumb">
          
<ol  class="breadcrumb-nav">
  

  

  

<li >
  <a href="http://localhost:1313/Hlunlun/">Home</a>
</li>


<li >
  <a href="http://localhost:1313/Hlunlun/posts/">Posts</a>
</li>


<li class="active">
  <a href="http://localhost:1313/Hlunlun/posts/attention/attention/">Calculate Attention</a>
</li>

</ol>




        </div>
      </div>
    
  </div>
</header>

    

<main id="main">
  <div class="container has-padding">
    <div class="article-card post single">
      <h1 class="title">Calculate Attention</h1>
      <div class="post-info">
        <span>



<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M32 456a24 24 0 0024 24h400a24 24 0 0024-24V176H32zm320-244a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm-80-80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm-80-80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm-80-80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zM456 64h-55.92V32h-48v32H159.92V32h-48v32H56a23.8 23.8 0 00-24 23.77V144h448V87.77A23.8 23.8 0 00456 64z'/></svg>













<time datetime=2024-12-22T00:31:04&#43;0800 class="date">December 22, 2024</time></span>
        <span>
















<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M256 48C141.13 48 48 141.13 48 256c0 114.69 93.32 208 208 208 114.86 0 208-93.14 208-208 0-114.69-93.31-208-208-208zm108 240H244a4 4 0 01-4-4V116a4 4 0 014-4h24a4 4 0 014 4v140h92a4 4 0 014 4v24a4 4 0 01-4 4z'/></svg>
3 mins to read</span>
        
          <span>












<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M256 256a112 112 0 10-112-112 112 112 0 00112 112zm0 32c-69.42 0-208 42.88-208 128v64h416v-64c0-85.12-138.58-128-208-128z'/></svg>




Lun</span>
        
        
        
          <span>posts </span>
        
      </div>
      <article class="post-entry content">
        
          <p>實作參考<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Annotated Transformer</a></p>
<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention<a hidden class="heading-anchor" aria-hidden="true" href="#scaled-dot-product-attention">#</a></h2>
<p>$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p>
<img src="https://hackmd.io/_uploads/rJuXRe24ye.png" style="margin:auto;display:block;">
<h3> 關於 \(\sqrt{d_k}\) </h3>
<ol>
<li>
<p>為何需要縮放因子 \(\sqrt{d_k}\) ? 這邊用例子簡單說明</p>
<ul>
<li>如果有一個值 <code>x[3]</code> 大其他值很多，經過softmax會將 <code>x</code> 範圍變到從0到1之間的 <code>y</code>，除了 <code>y[3]</code> 的其他值就會都趨近0(變得很不重要)，造成
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y: [0.0174, 0.0174, 0.0174, 0.9479]</span>
</span></span></code></pre></div></li>
<li>畫成圖可能就會長這樣
<img src="https://hackmd.io/_uploads/H1x1SlnN1g.png" width=300 style="display:block;"></li>
</ul>
</li>
<li>
<p>所以我們需要一個縮放因子縮小這個差異，這邊可以回憶一下<a href="https://zh.wikipedia.org/zh-tw/%E6%A0%87%E5%87%86%E5%8C%96_(%E7%BB%9F%E8%AE%A1%E5%AD%A6)">標準化</a>的原理，就是把資料分布改得較符合常態分佈，並縮小離群值對模型的影響，平均值為0，且標準差為1
$$Z = \frac{X-\mu}{\sigma} \sim N(0,1)$$</p>
<blockquote>
<p>&#x1f4a1; <a href="https://ithelp.ithome.com.tw/articles/10293893">標準化VS.正規化</a></p>
</blockquote>
</li>
<li>
<p>那這個重責大任為啥落到 \(\sqrt{d_k}\) 身上呢?</p>
<ul>
<li>
<p>先來看paper怎麼解釋: 假設 \(q\) 和 \(k\) 是常態分佈( \(\mu=0\)、\(\sigma=1\) )，那麼 \(q \cdot k\) 的 \(\mu=0\) 且 \(\sigma=d_k\) → 在說啥</p>
<blockquote>
<p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, \(q \cdot k = \textstyle\sum_{i=0}^{d_k} q_i k_i\) , has mean 0 and variance \(d_k\).</p>
</blockquote>
</li>
<li>
<p>先解決第一句:  <b>假設 \(q\) 和 \(k\) 是常態分佈( \(\mu=0\)、\(\sigma=1\) )</b></p>
  期望值 <span>\(E[q_i \cdot k_i] = 0\)</span>
<p>標準差</p>
  <p>\(Var(q_i k_i) = 1\)</p>
  <p>\(\implies E[q_i^2 k_i^2] - E^2[q_i k_i] = 1\)</p>
  <p>\(\implies E[q_i^2] E[k_i^2] - E^2[q_i] E^2[k_i] = 1\)</p>
  <p>\(\implies ( E[q_i^2] - E^2[q_i]) ( E[k_i^2] - E^2[k_i]) - E^2[q_i] E^2[k_i] = 1\)</p>
  <p>\(\implies Var(q_i) \ Var(k_i) - 0 = 1\)</p>
  <p>\(\implies Var(q_i) \ Var(k_i) = 1\)</p>
</li>
<li>
<p>再解決第一句: \(q \cdot k\) 的 \(\mu=0\) 且 \(\sigma=d_k\)</p>
  <p>$$
      \begin{aligned}
      E[q \cdot k] &= \textstyle\sum_{i=0}^{d_k} E[q_i k_i] \\
      &= d_k \times0 \\ 
      &=0
      \end{aligned}
  $$</p>
  <p>$$\begin{aligned}
  Var(q \cdot k)&=\textstyle\sum_{i=0}^{d_k} Var(q_i k_i)\\
  &= d_k \times1 \\
  &= d_k \\
  \end{aligned}$$</p>
</li>
<li>
<p>&#x1f389; 太棒了!破案了!
最後套上標準化的公式</p>
  <p>$$\begin{aligned}
  Z&=\cfrac{QK^T - E[QK^T]}{\sqrt{Var(QK^T)}} \\
  &=\cfrac{QK^T - 0}{\sqrt{d_k}}\\
  &= \cfrac{QK^T}{\sqrt{d_k}}
  \end{aligned}$$</p>
</li>
</ul>
<p>&#x1f4a1; 最後 <span>(\cfrac{QK^T}{\sqrt{d_k}})</span> 就可以使attention map經過 Softmax 梯度就不會被削弱了!</p>
</li>
</ol>
<ul>
<li>更詳細數學可以看
<ul>
<li>比較不同角度解釋: <a href="https://blog.csdn.net/ytusdc/article/details/121622205">为什么在进行softmax之前需要对attention进行scaled（为什么除以 $d_k$的平方根）</a></li>
<li>有推導數學(推推): <a href="https://allenwind.github.io/blog/16228/">分析與拓展：Transformer中的MultiHeadAttention為什麼要用scaled？</a></li>
</ul>
</li>
</ul>
<h3 id="如何知道-sqrtd_k-多大">如何知道 $\sqrt{d_k}$ 多大<a hidden class="heading-anchor" aria-hidden="true" href="#如何知道-sqrtd_k-多大">#</a></h3>
<ul>
<li>
<p>三個要關注的attention來源: query、value、key的維度可以當成都是一樣的，參考以下<a href="https://miro.medium.com/v2/resize:fit:828/format:webp/1*m58HPvaWXAt1bYNEHSwycA.png">這張圖</a>
<img src="https://hackmd.io/_uploads/r1I7hAoEkg.png" alt="image"></p>
</li>
<li>
<p>關於數學式視覺化後可以參考<a href="https://miro.medium.com/v2/resize:fit:828/format:webp/1*amnlT6Hjm5nV6NjFI0tRyg.png">這張圖</a>
<img src="https://hackmd.io/_uploads/S1E5nAj4ye.png" alt="image"></p>
</li>
<li>
<p>加上batch的維度，query、key、value變成tensor後的維度會長這樣</p>
<pre tabindex="0"><code>query: (batch_size, num_heads, seq_length, d_q)
key: (batch_size, num_heads, seq_length, d_k)
query: (batch_size, num_heads, seq_length, d_v)
</code></pre><blockquote>
<p>&#x1f4a1; <code>num_heads</code> 因為有多個Multi-Head Attention
<img src="https://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png" width=300></p>
</blockquote>
</li>
<li>
<p>所以獲取 \(d_k\) 的方式就是取得 <code>query</code> 的最後一個維度(反正 <code>d_q</code> = <code>d_k</code> = <code>d_v</code> )</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div></li>
</ul>
<h3 id="計算attention分數">計算Attention分數<a hidden class="heading-anchor" aria-hidden="true" href="#計算attention分數">#</a></h3>
<ul>
<li>Key的Transpose(可以去複習線代就大概知道這是一個怎麼回事)就是將最後一個維度和倒數第二個維度翻轉
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">-&gt;</span> <span class="n">key</span><span class="p">:</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>dot product
<ul>
<li>為何要用dot product來看相關性呢?參考<a href="https://zilliz.com/blog/similarity-metrics-for-vector-search">這張圖</a>
<img src="https://hackmd.io/_uploads/rkhfFWTEkl.png" alt="image"></li>
<li>
<p>\(\theta\) 越大 → dot product 越小 → 代表越不相關</p>
</li>
<li>
<p>\(\theta\) 越小 → dot product 越大 → 代表越相關</p>
</li>
</ul>
</li>
<li>接下來就是query和key、以及value做矩陣相乘算出attention分數，
<ul>
<li>
<p>我個人覺得可以這樣理解<br>
▶ query: 搜尋詞</p>
<p>▶ 搜出來的的資料，每個資料會長這樣</p>
<pre tabindex="0"><code>{key1: value1, key2: value2, ..., keyN:valueN}
key: 資料的標題
value: 資料內容
</code></pre><p>▶ 注意力機制就是要看哪個資料跟我的搜尋詞最相關，於是用dot product來看他們相關性的分數</p>
<pre tabindex="0"><code>attention = query * key
</code></pre><p>     ➜ attention 算出來就是每筆資料的標題和我搜尋的相關分數
<img src="https://hackmd.io/_uploads/S1_K613VJx.png" alt="image"><br>
     ➜ 分數越高，代表越接近我在搜尋的東西</p>
<p>▶ 最後在將這個分數乘上value，啥意思?<br>
     ➜ 給每筆資料打完相關性的分數後，在乘上資料內容就可以取得那些是重要資料內容了<br>
     ➜ 像這樣，把每筆資料的分數乘上資料內容，就可以知道如果分數較大，資料內容乘上分數就會比較大<br>
     ➜ 也代表著此內容較重要的意義，相反則同樣意義<br>
     ➜ 量化後就是一堆數字相乘，以下大概就是分數和資料內容矩陣相乘的樣子<br>
         <img src="https://hackmd.io/_uploads/H1He912Vkg.png" width=600>\</p>
</li>
<li>
<p>所以程式碼實現就是以下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">scores</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
</span></span></code></pre></div></li>
</ul>
<blockquote>
<p>&#x1f4a1; 為何是query與key相乘的細節可以參考<a href="https://www.youtube.com/watch?v=hYdO9CscNes">李宏毅的教材</a></p>
</blockquote>
</li>
</ul>
<h2 id="real-world-example">Real World Example<a hidden class="heading-anchor" aria-hidden="true" href="#real-world-example">#</a></h2>
<p>到 <a href="https://github.com/Hlunlun/Transformer/blob/master/example/scaled_dot_product.ipynb">GitHub</a> 看實際數據模擬</p>
<h2 id="implementation">Implementation<a hidden class="heading-anchor" aria-hidden="true" href="#implementation">#</a></h2>
<p>完整的 Scaled Dot-Product Attention 實作</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Compute &#39;Scaled Dot Product Attention&#39;
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>
</span></span></code></pre></div><h2 id="reference">Reference<a hidden class="heading-anchor" aria-hidden="true" href="#reference">#</a></h2>
<ul>
<li><a href="https://medium.com/@vmirly/tutorial-on-scaled-dot-product-attention-with-pytorch-implementation-from-scratch-66ed898bf817">Tutorial on Scaled Dot-Product Attention with PyTorch Implementation from Scratch</a></li>
<li><a href="https://ai.stackexchange.com/questions/41861/why-use-a-square-root-in-the-scaled-dot-product">Why use a &ldquo;square root&rdquo; in the scaled dot product</a></li>
</ul>



        
      </article>
    </div>

    
      <div class="meta article-card">
    <div class="row">
      <span class="label">



<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M32 456a24 24 0 0024 24h400a24 24 0 0024-24V176H32zm320-244a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm-80-80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm-80-80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm-80-80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zm0 80a4 4 0 014-4h40a4 4 0 014 4v40a4 4 0 01-4 4h-40a4 4 0 01-4-4zM456 64h-55.92V32h-48v32H159.92V32h-48v32H56a23.8 23.8 0 00-24 23.77V144h448V87.77A23.8 23.8 0 00456 64z'/></svg>













Published At</span><time>2024-12-22 00:31 CST</time>
      
    </div>

    
      <div class="row">
        <span class="label">













<svg xmlns='http://www.w3.org/2000/svg' class='ionicon' viewBox='0 0 512 512'><path d='M288 16L0 304l176 176 288-288V16zm80 128a32 32 0 1132-32 32 32 0 01-32 32z'/><path d='M480 64v144L216.9 471.1 242 496l270-272V64h-32z'/></svg>



Tagged with</span>
        <ul class="tags">
        
          <li class="hashed-tag"><a href="http://localhost:1313/Hlunlun/tags/attention">Attention</a></li>
        
        </ul>
      </div>
    

    

    
</div>

    

    

  </div>
</main>

    <footer id="footer">
  <div class="container has-padding is-flex">
    <ul class="links">
      
      <li>
        
        <a rel="nofollow" target="_blank" href="https://github.com/Hlunlun" title="Github">Github</a>
      </li>
      
      <li>
        <a
          rel="nofollow"
          target="_blank"
          href="https://github.com/wayjam/hugo-theme-fluency"
          title="using Hugo theme fluency">
          Theme Fluency
        </a>
      </li>
      <li>
        <a rel="nofollow" target="_blank" href="https://gohugo.io" title="Built with hugo">Built with Hugo</a>
      </li>
    </ul>
    <div class="copyright">
       &copy; 2025 Lun&#39;s
      
    </div>
  </div>
</footer>

<script>
    window.FluencyCopyIcon = '\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003csvg xmlns=\u0027http:\/\/www.w3.org\/2000\/svg\u0027 class=\u0027ionicon\u0027 viewBox=\u00270 0 512 512\u0027\u003e\u003crect x=\u0027128\u0027 y=\u0027128\u0027 width=\u0027336\u0027 height=\u0027336\u0027 rx=\u002757\u0027 ry=\u002757\u0027 stroke-linejoin=\u0027round\u0027 class=\u0027ionicon-fill-none ionicon-stroke-width\u0027\/\u003e\u003cpath d=\u0027M383.5 128l.5-24a56.16 56.16 0 00-56-56H112a64.19 64.19 0 00-64 64v216a56.16 56.16 0 0056 56h24\u0027 stroke-linecap=\u0027round\u0027 stroke-linejoin=\u0027round\u0027 class=\u0027ionicon-fill-none ionicon-stroke-width\u0027\/\u003e\u003c\/svg\u003e\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'
</script>


<script defer src="http://localhost:1313/Hlunlun/js/main.min.15ea6de828b83519cdc1bc66872563a50cd5e59b4b1cfc6f31019951922b2e78.js" integrity="sha256-Fept6Ci4NRnNwbxmhyVjpQzV5ZtLHPxvMQGZUZIrLng=" crossorigin="anonymous" async></script>


    <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
  integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
  crossorigin="anonymous"
/>


<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"
></script>


<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
  integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);"
></script>



<noscript>
<style type=text/css>#theme-selector-button{display:none}</style>
</noscript>




  </body>
</html>
