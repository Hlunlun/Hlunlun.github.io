<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss
  version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:wfw="http://wellformedweb.org/CommentAPI/"
  
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
  
>
  <channel>
    <title>Lun&#39;s Blog</title>
    <link>http://localhost:1313/Hlunlun/</link>
    <description>Recent content on Lun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 01:42:16 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/Hlunlun/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT 1.0: Improving Language Understanding by Generative Pre-Training.</title>
      <link>http://localhost:1313/Hlunlun/posts/gpt1/</link>
      <pubDate>Tue, 10 Dec 2024 01:42:16 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/posts/gpt1/</guid>
      <description>Radford, A., &amp;amp; Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training.</description>
      
        <content:encoded><![CDATA[<p>論文引用: Radford, A., &amp; Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training.</p>
]]></content:encoded>
      
    </item>
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
      <link>http://localhost:1313/Hlunlun/posts/bert/</link>
      <pubDate>Sun, 08 Dec 2024 23:18:01 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/posts/bert/</guid>
      <description>論文引用: Devlin, J., Chang, M., Lee, K., &amp;amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics.</description>
      
        <content:encoded><![CDATA[<p>論文引用: Devlin, J., Chang, M., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics.</p>
<p>因為GPT 1.0的發表，Google決定乘勝追擊，在2019推出BERT這個語言模型，相較於ELMo和GPT的下游單向的訓練方式，BERT用了雙向的Encoder，讓每個節點的到的上下文(context)資訊增加，當時的表現也是在多項語料庫上超越GPT1.0</p>
<br>
<h1 id="contextualized--embeddings">Contextualized  Embeddings</h1>
<p>同樣一個詞在不同語境下意義就會不同，所以比起以前的word vector一個蘿蔔一個坑，現在大家更關心的是如何量化前後文讓模型更能推敲出一個詞在不同語境的意思</p>
<p>tbc&hellip;</p>
<br>
<h1 id="pre-traning-tasks">Pre-traning Tasks</h1>
<p>用unlabed data(未標記、沒答案的資料)來訓練模型，未下游任務找到一個較好的初始點</p>
<h2 id="task-1-masked-lm">Task 1. Masked LM</h2>
<ul>
<li>
<p>Why</p>
<blockquote>
<p>Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.</p>
</blockquote>
<p>因為BERT用的是雙向的Encoder，這樣一來一個節點不就後面前面是啥都知道了嗎?對於QA這種task不就無法用了嗎?</p>
<p>所以，為了避免模型對於前面後面的context(上下文)搞混，用這種填充的訓練方式增強模型的了解文本的能力，這也是作者從<a href="https://gwern.net/doc/psychology/writing/1953-taylor.pdf">克漏字</a>得到的啟發，就是這麼神奇</p>
</li>
<li>
<p>How<br>
會遮蓋掉15%的token，遮蓋掉的部分會用特殊的<code>[MASK]</code>符號取代，模型只會關注被遮蓋的位置，經過12個encoder後，最後送到Softmax過濾，看哪個詞的機率最高的就是模型預測應該要放的詞</p>
<p>根據作者在論文中提到的BERT base(基礎版)預訓練任務畫成圖大概長以下這樣</p>
  <img src="base_structure.png" height=100 width =800 >
</li>
</ul>
<h2 id="task-2-next-sentence-prediction-nsp">Task 2. Next Sentence Prediction (NSP)</h2>
<p>就是字面上的意思，因為下游任務很多這種給模型一個句子，然後要模型分辨是正負面、entailment(文本大意)、similarity(相似度)等，為了在finetuned時有更好的表現，先用這個任務讓模型熟悉之後要做的事</p>
<ul>
<li>我也是沒想到模型就這麼聽話，真的比沒有NSP這個預訓練任務的模型表現好欸<br>
可以來看一下作者們做的消融實驗(ablatoin study)表格中，<code>LTR &amp; No NSP</code> 是left-to-right並且沒有NSP預訓練任務的模型(感覺就是在說GPT 1.0)，然後 <code>BiLSTM</code> 雙向的LSTM就很像在說ELMo，總而言之就是各種跟別人的比較(要凸顯自己很強)
<img src="ablation_study_nsp.png" height=100 width =500 style="display: block;"></li>
</ul>
<br>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://blog.csdn.net/qq_42791848/article/details/122374703">ELMo算法详解</a></li>
<li><a href="https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045">Learn how to build powerful contextual word embeddings with ELMo</a></li>
<li><a href="https://blog.csdn.net/weixin_46707326/article/details/123451774">浅谈feature-based 和 fine-tune</a></li>
<li><a href="https://github.com/salesforce/cove">CoVe GitHub</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/nlp/elmo">ELMo 一词多义</a></li>
<li><a href="https://medium.com/programming-with-data/31-elmo-embeddings-from-language-models-%E5%B5%8C%E5%85%A5%E5%BC%8F%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B-c59937da83af">31. ELMo (Embeddings from Language Models 嵌入式語言模型)</a></li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://quantpedia.com/bert-model-bidirectional-encoder-representations-from-transformers/">BERT Model – Bidirectional Encoder Representations from Transformers</a></li>
<li><a href="https://www.comet.com/site/blog/bert-state-of-the-art-model-for-natural-language-processing/">BERT: State-of-the-Art Model for Natural Language Processing</a></li>
</ul>
]]></content:encoded>
      
    </item>
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>http://localhost:1313/Hlunlun/posts/llama/</link>
      <pubDate>Sun, 08 Dec 2024 21:55:50 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/posts/llama/</guid>
      <description>論文引用: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp;amp; Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971.</description>
      
        <content:encoded><![CDATA[<p>論文引用: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971.
<img src="llama_milestone.png" height=100 width =1000 style=" margin: auto; display: block;">
<br></p>
<h1 id="challenge-scaling-law">Challenge Scaling Law</h1>
<h2 id="scaling-law">Scaling Law</h2>
<p>先說甚麼是Scaling Law</p>
<p>關於更詳細的Scaling Law可以參考這篇<a href="https://arxiv.org/abs/2001.08361">論文</a></p>
<h2 id="羊駝的覺醒">羊駝的覺醒</h2>
<ul>
<li>
<p>論文中提到: <strong>LLM with fast inference rather than a fast training process</strong>，以前會考慮到scaling law是因為想要訓練的時間短一點，但是訓練時間短對於LLM的使用並沒有幫助，我們想要的是在使用LLM時可以更快速的得到想要的回答 &ndash; 也就是在inference時快一點，在訓練時慢一點沒差</p>
</li>
<li>
<p>那要怎麼讓參數小於GPT 十倍之多的llama 1.0有較好的表現呢?就是給他訓練資料多一點，訓練時常久一點，即使是小模型也能在多次訓練後有較好的表現!
<img src="scaling_law.png" height=100 width=1000 style=" margin: auto; display: block;"></p>
</li>
</ul>
<h1 id="results">Results</h1>
<ul>
<li>雖然參數少很多，但是在許多與料庫上的表現都優於GPT
<img src="results_1.png" height=100 width=1000 style=" margin: auto; display: block;">
<img src="results_2.png" height=100 width=1000 style=" margin: auto; display: block;"></li>
</ul>
]]></content:encoded>
      
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/Hlunlun/about/</link>
      <pubDate>Sun, 08 Dec 2024 21:37:49 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/about/</guid>
      <description>A computer science student at National Cheng Kung University, dedicated to research in the field of large language models.</description>
      
        <content:encoded><![CDATA[<p>A computer science student at National Cheng Kung University, dedicated to research in the field of large language models.</p>
]]></content:encoded>
      
    </item>
  </channel>
</rss>
