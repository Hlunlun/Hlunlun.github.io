<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss
  version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:wfw="http://wellformedweb.org/CommentAPI/"
  
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
  
>
  <channel>
    <title>Lun&#39;s Blog</title>
    <link>http://localhost:1313/Hlunlun/</link>
    <description>Recent content on Lun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Dec 2024 23:18:01 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/Hlunlun/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
      <link>http://localhost:1313/Hlunlun/posts/bert/</link>
      <pubDate>Sun, 08 Dec 2024 23:18:01 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/posts/bert/</guid>
      <description>論文引用: Devlin, J., Chang, M., Lee, K., &amp;amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics.</description>
      
        <content:encoded><![CDATA[<p>論文引用: Devlin, J., Chang, M., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics.</p>
<p>因為GPT 1.0的發表，Google決定乘勝追擊，在2019推出BERT這個語言模型，相較於ELMo和GPT的下游單向的訓練方式，BERT用了雙向的Encoder，讓每個節點的到的上下文(context)資訊增加，當時的表現也是在多項語料庫上超越GPT1.0</p>
<br>
<h1 id="pre-traning-tasks">Pre-traning Tasks</h1>
<h2 id="task-1-masked-lm">Task 1. Masked LM</h2>
<ul>
<li>Why</li>
<li>How</li>
<li>會遮蓋掉15%的token，遮蓋掉的部分會用特殊的<code>[MASK]</code>符號取代，模型只會關注
根據作者在論文中提到的預訓練任務畫成圖大概長以下這樣</li>
</ul>
<img src="base_structure.png" height=100 width =1000 style=" margin: auto; display: block;">
<h2 id="task-2-next-sentence-prediction-nsp">Task 2. Next Sentence Prediction (NSP)</h2>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://blog.csdn.net/qq_42791848/article/details/122374703">ELMo算法详解</a></li>
<li><a href="https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045">Learn how to build powerful contextual word embeddings with ELMo</a></li>
<li><a href="https://blog.csdn.net/weixin_46707326/article/details/123451774">浅谈feature-based 和 fine-tune</a></li>
<li><a href="https://github.com/salesforce/cove">CoVe GitHub</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/nlp/elmo">ELMo 一词多义</a></li>
<li><a href="https://medium.com/programming-with-data/31-elmo-embeddings-from-language-models-%E5%B5%8C%E5%85%A5%E5%BC%8F%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B-c59937da83af">31. ELMo (Embeddings from Language Models 嵌入式語言模型)</a></li>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://quantpedia.com/bert-model-bidirectional-encoder-representations-from-transformers/">BERT Model – Bidirectional Encoder Representations from Transformers</a></li>
<li><a href="https://www.comet.com/site/blog/bert-state-of-the-art-model-for-natural-language-processing/">BERT: State-of-the-Art Model for Natural Language Processing</a></li>
</ul>
]]></content:encoded>
      
    </item>
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>http://localhost:1313/Hlunlun/posts/llama/</link>
      <pubDate>Sun, 08 Dec 2024 21:55:50 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/posts/llama/</guid>
      <description>論文引用: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp;amp; Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971.</description>
      
        <content:encoded><![CDATA[<p>論文引用: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971.
<img src="llama_milestone.png" height=100 width =1000 style=" margin: auto; display: block;">
<br></p>
<h1 id="challenge-scaling-law">Challenge Scaling Law</h1>
<h2 id="scaling-law">Scaling Law</h2>
<p>先說甚麼是Scaling Law</p>
<p>關於更詳細的Scaling Law可以參考這篇<a href="https://arxiv.org/abs/2001.08361">論文</a></p>
<h2 id="羊駝的覺醒">羊駝的覺醒</h2>
<ul>
<li>
<p>論文中提到: <strong>LLM with fast inference rather than a fast training process</strong>，以前會考慮到scaling law是因為想要訓練的時間短一點，但是訓練時間短對於LLM的使用並沒有幫助，我們想要的是在使用LLM時可以更快速的得到想要的回答 &ndash; 也就是在inference時快一點，在訓練時慢一點沒差</p>
</li>
<li>
<p>那要怎麼讓參數小於GPT 十倍之多的llama 1.0有較好的表現呢?就是給他訓練資料多一點，訓練時常久一點，即使是小模型也能在多次訓練後有較好的表現!
<img src="scaling_law.png" height=100 width=1000 style=" margin: auto; display: block;"></p>
</li>
</ul>
<h1 id="results">Results</h1>
<ul>
<li>雖然參數少很多，但是在許多與料庫上的表現都優於GPT
<img src="results_1.png" height=100 width=1000 style=" margin: auto; display: block;">
<img src="results_2.png" height=100 width=1000 style=" margin: auto; display: block;"></li>
</ul>
]]></content:encoded>
      
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/Hlunlun/about/</link>
      <pubDate>Sun, 08 Dec 2024 21:37:49 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/about/</guid>
      <description>A computer science student at National Cheng Kung University, dedicated to research in the field of large language models.</description>
      
        <content:encoded><![CDATA[<p>A computer science student at National Cheng Kung University, dedicated to research in the field of large language models.</p>
]]></content:encoded>
      
    </item>
  </channel>
</rss>
