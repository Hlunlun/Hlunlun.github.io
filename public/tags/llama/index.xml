<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss
  version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:wfw="http://wellformedweb.org/CommentAPI/"
  
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
  
>
  <channel>
    <title>Llama on Lun&#39;s</title>
    <link>http://localhost:1313/Hlunlun/tags/llama/</link>
    <description>Recent content in Llama on Lun&#39;s</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Dec 2024 21:55:50 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/Hlunlun/tags/llama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>http://localhost:1313/Hlunlun/posts/llama/</link>
      <pubDate>Sun, 08 Dec 2024 21:55:50 +0800</pubDate>
      <guid>http://localhost:1313/Hlunlun/posts/llama/</guid>
      <description>論文引用: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp;amp; Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971.</description>
      
        <content:encoded><![CDATA[<p>論文引用: Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. ArXiv, abs/2302.13971.
<img src="llama_milestone.png" width =1000 style=" margin: auto; display: block;">
<br></p>
<h1 id="challenge-scaling-law">Challenge Scaling Law</h1>
<h2 id="scaling-law">Scaling Law</h2>
<p>先說甚麼是Scaling Law</p>
<p>關於更詳細的Scaling Law可以參考這篇<a href="https://arxiv.org/abs/2001.08361">論文</a></p>
<h2 id="羊駝的覺醒">羊駝的覺醒</h2>
<ul>
<li>
<p>論文中提到: <strong>LLM with fast inference rather than a fast training process</strong>，以前會考慮到scaling law是因為想要訓練的時間短一點，但是訓練時間短對於LLM的使用並沒有幫助，我們想要的是在使用LLM時可以更快速的得到想要的回答 &ndash; 也就是在inference時快一點，在訓練時慢一點沒差</p>
</li>
<li>
<p>那要怎麼讓參數小於GPT 十倍之多的llama 1.0有較好的表現呢?就是給他訓練資料多一點，訓練時常久一點，即使是小模型也能在多次訓練後有較好的表現!
<img src="scaling_law.png" height=100 width=1000 style=" margin: auto; display: block;"></p>
</li>
</ul>
<h1 id="results">Results</h1>
<ul>
<li>雖然參數少很多，但是在許多與料庫上的表現都優於GPT
<img src="results_1.png" height=100 width=1000 style=" margin: auto; display: block;">
<img src="results_2.png" height=100 width=1000 style=" margin: auto; display: block;"></li>
</ul>
]]></content:encoded>
      
    </item>
  </channel>
</rss>
